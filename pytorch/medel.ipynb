{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db03e4a0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f4413b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:47:43.579830Z",
     "start_time": "2021-04-28T02:47:43.064542Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import AlexNet\n",
    "from torchviz import make_dot\n",
    "import torch.nn as nn\n",
    "CHANNELS = [20, 50, 500]\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696a3b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:47:48.195547Z",
     "start_time": "2021-04-28T02:47:48.081622Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 56, 56])\n",
      "torch.Size([1, 20, 28, 28])\n",
      "torch.Size([1, 50, 24, 24])\n",
      "torch.Size([1, 50, 12, 12])\n",
      "torch.Size([1, 7200])\n",
      "torch.Size([1, 500])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "input=torch.rand(1,12,60,60)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, CHANNELS[0], 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(CHANNELS[0], CHANNELS[1], 5)\n",
    "        self.fc1 = nn.Linear(CHANNELS[1] * 12 * 12, CHANNELS[2])\n",
    "        self.fc2 = nn.Linear(CHANNELS[2], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(x.shape)\n",
    "        x=self.pool(x)\n",
    "        print(x.shape)\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(x.shape)\n",
    "        x=self.pool(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "model=Net(12)\n",
    "y=model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6608bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:47:53.194797Z",
     "start_time": "2021-04-28T02:47:53.191671Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g=make_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad310b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:47:54.316139Z",
     "start_time": "2021-04-28T02:47:54.272763Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'espnet_model.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.render('espnet_model', view=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb4079f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:47:55.533891Z",
     "start_time": "2021-04-28T02:47:55.528205Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该层的结构：[20, 12, 5, 5]\n",
      "该层参数和：6000\n",
      "该层的结构：[20]\n",
      "该层参数和：20\n",
      "该层的结构：[50, 20, 5, 5]\n",
      "该层参数和：25000\n",
      "该层的结构：[50]\n",
      "该层参数和：50\n",
      "该层的结构：[500, 7200]\n",
      "该层参数和：3600000\n",
      "该层的结构：[500]\n",
      "该层参数和：500\n",
      "该层的结构：[2, 500]\n",
      "该层参数和：1000\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "总参数数量和：3632572\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "        l = 1\n",
    "        print(\"该层的结构：\" + str(list(i.size())))\n",
    "        for j in i.size():\n",
    "                l *= j\n",
    "        print(\"该层参数和：\" + str(l))\n",
    "        k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e70c66",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 实验1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2df155ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-26T08:06:37.724806Z",
     "start_time": "2021-04-26T08:06:37.603650Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 50, 12, 12])\n",
      "torch.Size([64, 2, 12, 12])\n",
      "该层的结构：[20, 12, 5, 5]\n",
      "该层参数和：6000\n",
      "该层的结构：[20]\n",
      "该层参数和：20\n",
      "该层的结构：[50, 20, 5, 5]\n",
      "该层参数和：25000\n",
      "该层的结构：[50]\n",
      "该层参数和：50\n",
      "该层的结构：[25, 50, 1, 1]\n",
      "该层参数和：1250\n",
      "该层的结构：[25]\n",
      "该层参数和：25\n",
      "该层的结构：[50, 25, 1, 1]\n",
      "该层参数和：1250\n",
      "该层的结构：[50]\n",
      "该层参数和：50\n",
      "该层的结构：[2, 50, 1, 1]\n",
      "该层参数和：100\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "总参数数量和：33747\n"
     ]
    }
   ],
   "source": [
    "input=torch.rand(64,12,60,60)\n",
    "class one_one_Net(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(one_one_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, CHANNELS[0], 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(CHANNELS[0], CHANNELS[1], 5)\n",
    "\n",
    "        #         self.fc1 = nn.Linear(CHANNELS[1] * 12 * 12, CHANNELS[2])\n",
    "        self.softconv1 = nn.Conv2d(in_channels=50, out_channels=25, kernel_size=1, stride=1, padding=0)\n",
    "        self.softconv2 = nn.Conv2d(in_channels=25, out_channels=50, kernel_size=1, stride=1, padding=0)\n",
    "        self.softconv3 = nn.Conv2d(in_channels=50, out_channels=2, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    #         self.fc2 =nn.Linear(1,CHANNELS[2])\n",
    "    #         self.fc3 = nn.Linear(CHANNELS[2], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        print(x.shape)\n",
    "        #         x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "\n",
    "        #         x = F.relu(self.fc1(x))\n",
    "        x = self.softconv1(x)\n",
    "        x = self.softconv2(x)\n",
    "        x = self.softconv3(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "model=one_one_Net(12)\n",
    "y=model(input)\n",
    "g=make_dot(y)\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "        l = 1\n",
    "        print(\"该层的结构：\" + str(list(i.size())))\n",
    "        for j in i.size():\n",
    "                l *= j\n",
    "        print(\"该层参数和：\" + str(l))\n",
    "        k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3f24c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 实验2    nin  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8f0898",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:50:31.490168Z",
     "start_time": "2021-04-28T02:50:30.946670Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape:  torch.Size([64, 96, 13, 13])\n",
      "1 output shape:  torch.Size([64, 96, 12, 12])\n",
      "2 output shape:  torch.Size([64, 256, 12, 12])\n",
      "3 output shape:  torch.Size([64, 256, 11, 11])\n",
      "4 output shape:  torch.Size([64, 384, 11, 11])\n",
      "5 output shape:  torch.Size([64, 384, 10, 10])\n",
      "6 output shape:  torch.Size([64, 384, 10, 10])\n",
      "7 output shape:  torch.Size([64, 2, 10, 10])\n",
      "8 output shape:  torch.Size([64, 2, 1, 1])\n",
      "9 output shape:  torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "# import d2lzh_pytorch as d2l\n",
    "\n",
    "\n",
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU())\n",
    "    return blk\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "    \n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x): # x shape: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nin_block(12, 96, kernel_size=11, stride=4, padding=0),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1), \n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 2, kernel_size=3, stride=1, padding=1),\n",
    "    GlobalAvgPool2d(),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "    FlattenLayer())\n",
    "# X = torch.rand(64,12,60,60)\n",
    "X = torch.rand(64, 12, 60, 60)\n",
    "for name, blk in net.named_children():\n",
    "    \n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8baddc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# nin网络  加入bn层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab547ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:50:44.229096Z",
     "start_time": "2021-04-28T02:50:43.659299Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape:  torch.Size([64, 96, 13, 13])\n",
      "1 output shape:  torch.Size([64, 96, 12, 12])\n",
      "2 output shape:  torch.Size([64, 256, 12, 12])\n",
      "3 output shape:  torch.Size([64, 256, 11, 11])\n",
      "4 output shape:  torch.Size([64, 384, 11, 11])\n",
      "5 output shape:  torch.Size([64, 384, 10, 10])\n",
      "6 output shape:  torch.Size([64, 384, 10, 10])\n",
      "7 output shape:  torch.Size([64, 2, 10, 10])\n",
      "8 output shape:  torch.Size([64, 2, 1, 1])\n",
      "9 output shape:  torch.Size([64, 2])\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "该层的结构：[96, 12, 11, 11]\n",
      "该层参数和：139392\n",
      "该层的结构：[96]\n",
      "该层参数和：96\n",
      "该层的结构：[1, 96, 1, 1]\n",
      "该层参数和：96\n",
      "该层的结构：[1, 96, 1, 1]\n",
      "该层参数和：96\n",
      "该层的结构：[96, 96, 1, 1]\n",
      "该层参数和：9216\n",
      "该层的结构：[96]\n",
      "该层参数和：96\n",
      "该层的结构：[96, 96, 1, 1]\n",
      "该层参数和：9216\n",
      "该层的结构：[96]\n",
      "该层参数和：96\n",
      "该层的结构：[256, 96, 5, 5]\n",
      "该层参数和：614400\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[1, 256, 1, 1]\n",
      "该层参数和：256\n",
      "该层的结构：[1, 256, 1, 1]\n",
      "该层参数和：256\n",
      "该层的结构：[256, 256, 1, 1]\n",
      "该层参数和：65536\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256, 256, 1, 1]\n",
      "该层参数和：65536\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[384, 256, 3, 3]\n",
      "该层参数和：884736\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[1, 384, 1, 1]\n",
      "该层参数和：384\n",
      "该层的结构：[1, 384, 1, 1]\n",
      "该层参数和：384\n",
      "该层的结构：[384, 384, 1, 1]\n",
      "该层参数和：147456\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[384, 384, 1, 1]\n",
      "该层参数和：147456\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[2, 384, 3, 3]\n",
      "该层参数和：6912\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "该层的结构：[1, 2, 1, 1]\n",
      "该层参数和：2\n",
      "该层的结构：[1, 2, 1, 1]\n",
      "该层参数和：2\n",
      "该层的结构：[2, 2, 1, 1]\n",
      "该层参数和：4\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "该层的结构：[2, 2, 1, 1]\n",
      "该层参数和：4\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "总参数数量和：2093554\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "# import d2lzh_pytorch as d2l\n",
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training,\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n",
    "\n",
    "\n",
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                        BatchNorm(out_channels, num_dims=4),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU())\n",
    "    return blk\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "    \n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x): # x shape: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nin_block(12, 96, kernel_size=11, stride=4, padding=0),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1), \n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 2, kernel_size=3, stride=1, padding=1),\n",
    "    GlobalAvgPool2d(),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "    FlattenLayer())\n",
    "# X = torch.rand(64,12,60,60)\n",
    "X = torch.rand(64, 12, 60, 60)\n",
    "for name, blk in net.named_children():\n",
    "    \n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)\n",
    "print(200*'-')\n",
    "model= net\n",
    "\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "        l = 1\n",
    "        print(\"该层的结构：\" + str(list(i.size())))\n",
    "        for j in i.size():\n",
    "                l *= j\n",
    "        print(\"该层参数和：\" + str(l))\n",
    "        k = k + l\n",
    "print(\"总参数数量和：\" + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b3d60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T13:17:10.680341Z",
     "start_time": "2021-04-25T13:17:10.292177Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# con1 conv2  nin_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955eb5ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:49:18.340349Z",
     "start_time": "2021-04-28T02:49:17.620597Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该层的结构：[20, 12, 5, 5]\n",
      "该层参数和：6000\n",
      "该层的结构：[20]\n",
      "该层参数和：20\n",
      "该层的结构：[50, 20, 5, 5]\n",
      "该层参数和：25000\n",
      "该层的结构：[50]\n",
      "该层参数和：50\n",
      "该层的结构：[384, 50, 3, 3]\n",
      "该层参数和：172800\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[1, 384, 1, 1]\n",
      "该层参数和：384\n",
      "该层的结构：[1, 384, 1, 1]\n",
      "该层参数和：384\n",
      "该层的结构：[384, 384, 1, 1]\n",
      "该层参数和：147456\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[384, 384, 1, 1]\n",
      "该层参数和：147456\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[2, 384, 3, 3]\n",
      "该层参数和：6912\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "该层的结构：[1, 2, 1, 1]\n",
      "该层参数和：2\n",
      "该层的结构：[1, 2, 1, 1]\n",
      "该层参数和：2\n",
      "该层的结构：[2, 2, 1, 1]\n",
      "该层参数和：4\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "该层的结构：[2, 2, 1, 1]\n",
      "该层参数和：4\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "总参数数量和：507632\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import AlexNet\n",
    "from torchviz import make_dot\n",
    "import torch.nn as nn\n",
    "CHANNELS = [20, 50, 500]\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "input=torch.rand(64,12,60,60)\n",
    "\n",
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training,\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n",
    "\n",
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                        BatchNorm(out_channels, num_dims=4),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU())\n",
    "    return blk\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):  # x shape: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    #     nin_block(12, 96, kernel_size=11, stride=4, padding=0),\n",
    "    #     nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    #     nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "\n",
    "    #     nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nn.Conv2d(12, 20, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(20, 50, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    nin_block(50, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 2, kernel_size=3, stride=1, padding=1),\n",
    "    GlobalAvgPool2d(),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "    FlattenLayer())\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# class fcl2one_Net(nn.Module):\n",
    "#     def __init__(self, input_channels):\n",
    "#         super(fcl2one_Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(input_channels, CHANNELS[0], 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(CHANNELS[0], CHANNELS[1], 5)\n",
    "        \n",
    "#         nin_block(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "# #         self.fc1 = nn.Linear(CHANNELS[1] * 12 * 12, CHANNELS[2])\n",
    "# #         self.fc2 = nn.Linear(CHANNELS[2], 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "#         x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "        \n",
    "#         x = self.fc2(x)\n",
    "#         print(x.shape)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "# model=one_one_Net(12)\n",
    "\n",
    "# y=model(input)\n",
    "model=net\n",
    "y=net(input)\n",
    "g=make_dot(y)\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "        l = 1\n",
    "        print(\"该层的结构：\" + str(list(i.size())))\n",
    "        for j in i.size():\n",
    "                l *= j\n",
    "        print(\"该层参数和：\" + str(l))\n",
    "        k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880fd5e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T13:31:38.216565Z",
     "start_time": "2021-04-27T13:31:37.466499Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to export an ONNX attribute 'onnx::Gather', since it's not constant, please try to make things (e.g., kernel size) static if possible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a0801e39125c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# # 其实就两句话\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/tensorwatch/__init__.py\u001b[0m in \u001b[0;36mdraw_model\u001b[0;34m(model, input_shape, orientation, png_filename)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpng_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#orientation = 'LR' for landscpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddenlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_draw_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytorch_draw_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/tensorwatch/model_graph/hiddenlayer/pytorch_draw_model.py\u001b[0m in \u001b[0;36mdraw_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_img_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDotWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/tensorwatch/model_graph/hiddenlayer/pytorch_draw_model.py\u001b[0m in \u001b[0;36mdraw_img_classifier\u001b[0;34m(model, dataset, display_param_nodes, rankdir, styles, input_shape)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mnon_para_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistiller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_non_parallel_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_para_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msgraph2dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_param_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/tensorwatch/model_graph/hiddenlayer/summary_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, dummy_input, apply_scope_name_workarounds)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# Let ONNX do the heavy lifting: fusing the convolution nodes; fusing the nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# composing a GEMM operation; etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorExportTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONNX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m_optimize_trace\u001b[0;34m(graph, operator_export_type)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_optimize_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_erase_number_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m_run_symbolic_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_run_symbolic_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_symbolic_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_run_symbolic_function\u001b[0;34m(g, n, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m    655\u001b[0m                                   .format(op_name, opset_version, op_name))\n\u001b[1;32m    656\u001b[0m                 \u001b[0mop_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msym_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_registered_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"prim\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(g, *args)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# some args may be optional, so the length may be smaller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_descriptors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_parse_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_desc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_descriptors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# In Python 2 functools.wraps chokes on partially applied functions, so we need this as a workaround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# some args may be optional, so the length may be smaller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_descriptors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_parse_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_desc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_descriptors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# In Python 2 functools.wraps chokes on partially applied functions, so we need this as a workaround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wuxr/anaconda3/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py\u001b[0m in \u001b[0;36m_parse_arg\u001b[0;34m(value, desc)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'onnx::Constant'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     raise RuntimeError(\"Failed to export an ONNX attribute '\" + v.node().kind() +\n\u001b[0;32m---> 81\u001b[0;31m                                        \u001b[0;34m\"', since it's not constant, please try to make \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                                        \"things (e.g., kernel size) static if possible\")\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to export an ONNX attribute 'onnx::Gather', since it's not constant, please try to make things (e.g., kernel size) static if possible"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import tensorwatch as tw\n",
    "\n",
    " \n",
    "# # # 其实就两句话\n",
    "# model=net\n",
    "# tw.draw_model(model, [64, 12, 60, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075feda",
   "metadata": {},
   "source": [
    "# one_conv_one_conv    三个（卷积+batchnormalization+1*1卷积）+fl层（用他）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2979f3f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-27T02:28:59.570469Z",
     "start_time": "2021-04-27T02:28:59.183046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该层的结构：[96, 12, 11, 11]\n",
      "该层参数和：139392\n",
      "该层的结构：[96]\n",
      "该层参数和：96\n",
      "该层的结构：[96, 96, 1, 1]\n",
      "该层参数和：9216\n",
      "该层的结构：[96]\n",
      "该层参数和：96\n",
      "该层的结构：[256, 96, 5, 5]\n",
      "该层参数和：614400\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256, 256, 1, 1]\n",
      "该层参数和：65536\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[384, 256, 3, 3]\n",
      "该层参数和：884736\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[384, 384, 1, 1]\n",
      "该层参数和：147456\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[50, 384, 1, 1]\n",
      "该层参数和：19200\n",
      "该层的结构：[50]\n",
      "该层参数和：50\n",
      "该层的结构：[50, 7200]\n",
      "该层参数和：360000\n",
      "该层的结构：[50]\n",
      "该层参数和：50\n",
      "该层的结构：[2, 50]\n",
      "该层参数和：100\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "总参数数量和：2241610\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import AlexNet\n",
    "from torchviz import make_dot\n",
    "import torch.nn as nn\n",
    "CHANNELS = [20, 50, 500]\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "input=torch.rand(64,12,60,60)\n",
    "\n",
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training,\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n",
    "\n",
    "class one_fl_Net_con_nomal(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(one_fl_Net_con_nomal, self).__init__()\n",
    "\n",
    "        self.a_1 = nn.Conv2d(12, 96, kernel_size=11, stride=4, padding=0)\n",
    "        self.batch_1 = BatchNorm(96, num_dims=4)\n",
    "        self.a_one = nn.Conv2d(96, 96, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.b_1 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.batch_2 = BatchNorm(256, num_dims=4)\n",
    "        self.b_one = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.c_1 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_3 = BatchNorm(384, num_dims=4)\n",
    "        self.c_one = nn.Conv2d(384, 384, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.consoft = nn.Conv2d(384, 50, kernel_size=1, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(50 * 12 * 12, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch_1(self.a_1(x)))\n",
    "        x = F.relu(self.a_one(F.relu(self.a_one(x))))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.batch_2(self.b_1(x)))\n",
    "        x = F.relu(self.b_one(F.relu(self.b_one(x))))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.batch_3(self.c_1(x)))\n",
    "        x = F.relu(self.c_one(F.relu(self.c_one(x))))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.consoft(x)\n",
    "\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        print('aaaaa'+x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model=one_one_Net(12)\n",
    "y=model(input)\n",
    "g=make_dot(y)\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "        l = 1\n",
    "        print(\"该层的结构：\" + str(list(i.size())))\n",
    "        for j in i.size():\n",
    "                l *= j\n",
    "        print(\"该层参数和：\" + str(l))\n",
    "        k = k + l\n",
    "print(\"总参数数量和：\" + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f1932",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# one_conv_one_conv    三个（卷积+BN+1*1卷积）+fl层+BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c74fae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T02:48:22.824294Z",
     "start_time": "2021-04-28T02:48:22.017601Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 96, 12, 12])\n",
      "torch.Size([64, 256, 11, 11])\n",
      "torch.Size([64, 384, 10, 10])\n",
      "torch.Size([64, 50, 12, 12])\n",
      "该层的结构：[96, 12, 11, 11]\n",
      "该层参数和：139392\n",
      "该层的结构：[96]\n",
      "该层参数和：96\n",
      "该层的结构：[1, 96, 1, 1]\n",
      "该层参数和：96\n",
      "该层的结构：[1, 96, 1, 1]\n",
      "该层参数和：96\n",
      "该层的结构：[96, 96, 1, 1]\n",
      "该层参数和：9216\n",
      "该层的结构：[96]\n",
      "该层参数和：96\n",
      "该层的结构：[256, 96, 5, 5]\n",
      "该层参数和：614400\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[1, 256, 1, 1]\n",
      "该层参数和：256\n",
      "该层的结构：[1, 256, 1, 1]\n",
      "该层参数和：256\n",
      "该层的结构：[256, 256, 1, 1]\n",
      "该层参数和：65536\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[384, 256, 3, 3]\n",
      "该层参数和：884736\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[1, 384, 1, 1]\n",
      "该层参数和：384\n",
      "该层的结构：[1, 384, 1, 1]\n",
      "该层参数和：384\n",
      "该层的结构：[384, 384, 1, 1]\n",
      "该层参数和：147456\n",
      "该层的结构：[384]\n",
      "该层参数和：384\n",
      "该层的结构：[50, 384, 1, 1]\n",
      "该层参数和：19200\n",
      "该层的结构：[50]\n",
      "该层参数和：50\n",
      "该层的结构：[50, 7200]\n",
      "该层参数和：360000\n",
      "该层的结构：[50]\n",
      "该层参数和：50\n",
      "该层的结构：[1, 50]\n",
      "该层参数和：50\n",
      "该层的结构：[1, 50]\n",
      "该层参数和：50\n",
      "该层的结构：[2, 50]\n",
      "该层参数和：100\n",
      "该层的结构：[2]\n",
      "该层参数和：2\n",
      "总参数数量和：2243182\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import AlexNet\n",
    "from torchviz import make_dot\n",
    "import torch.nn as nn\n",
    "CHANNELS = [20, 50, 500]\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "input=torch.rand(64,12,60,60)\n",
    "\n",
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training,\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n",
    "\n",
    "class one_fl_Net_con_bn_fl_bn(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(one_fl_Net_con_bn_fl_bn, self).__init__()\n",
    "\n",
    "        self.a_1 = nn.Conv2d(12, 96, kernel_size=11, stride=4, padding=0)\n",
    "        self.batch_1 = BatchNorm(96, num_dims=4)\n",
    "        self.a_one = nn.Conv2d(96, 96, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.b_1 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.batch_2 = BatchNorm(256, num_dims=4)\n",
    "        self.b_one = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.c_1 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_3 = BatchNorm(384, num_dims=4)\n",
    "        self.c_one = nn.Conv2d(384, 384, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.consoft = nn.Conv2d(384, 50, kernel_size=1, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(50 * 12 * 12, 50)\n",
    "        self.batch_fl = BatchNorm(50, num_dims=2)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch_1(self.a_1(x)))\n",
    "        x = F.relu(self.a_one(F.relu(self.a_one(x))))\n",
    "        x = self.pool(x)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.batch_2(self.b_1(x)))\n",
    "        x = F.relu(self.b_one(F.relu(self.b_one(x))))\n",
    "        x = self.pool(x)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.batch_3(self.c_1(x)))\n",
    "        x = F.relu(self.c_one(F.relu(self.c_one(x))))\n",
    "        x = self.pool(x)\n",
    "        print(x.shape)\n",
    "        x = self.consoft(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        x = F.relu(self.batch_fl(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model=one_fl_Net_con_bn_fl_bn(12)\n",
    "y=model(input)\n",
    "g=make_dot(y)\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "        l = 1\n",
    "        print(\"该层的结构：\" + str(list(i.size())))\n",
    "        for j in i.size():\n",
    "                l *= j\n",
    "        print(\"该层参数和：\" + str(l))\n",
    "        k = k + l\n",
    "print(\"总参数数量和：\" + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ce0e43",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# noe_fcl_Net          三个（卷积+1*1卷积）+fl层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40cd4c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import AlexNet\n",
    "from torchviz import make_dot\n",
    "import torch.nn as nn\n",
    "CHANNELS = [20, 50, 500]\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "input=torch.rand(64,12,60,60)\n",
    "\n",
    "\n",
    "\n",
    "class one_fl_Net(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(one_fl_Net, self).__init__()\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(input_channels, CHANNELS[0], 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.a_1 = nn.Conv2d(12, 96, kernel_size=11, stride=4, padding=0)\n",
    "        self.a_one = nn.Conv2d(96, 96, kernel_size=1)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        \n",
    "        self.b_1 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.b_one = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        \n",
    "        self.c_1 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.c_one = nn.Conv2d(384, 384, kernel_size=1)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        \n",
    "        self.consoft = nn.Conv2d(384,50,kernel_size=1,stride=1,padding=1)\n",
    "        self.fc1 = nn.Linear(50 * 12 * 12, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "               \n",
    "        x = F.relu(self.a_1(x))\n",
    "        x = F.relu(self.a_one(F.relu(self.a_one(x))))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.b_1(x))\n",
    "        x = F.relu(self.b_one(F.relu(self.b_one(x))))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.c_1(x))\n",
    "        x = F.relu(self.c_one(F.relu(self.c_one(x))))              \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.consoft(x)   \n",
    "\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model=one_one_Net(12)\n",
    "y=model(input)\n",
    "g=make_dot(y)\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "        l = 1\n",
    "        print(\"该层的结构：\" + str(list(i.size())))\n",
    "        for j in i.size():\n",
    "                l *= j\n",
    "        print(\"该层参数和：\" + str(l))\n",
    "        k = k + l\n",
    "print(\"总参数数量和：\" + str(k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
