{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb8ba08",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04a8338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T13:02:56.071401Z",
     "start_time": "2021-04-28T13:02:48.992247Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Loading test data ...\n",
      "Loaded images of shape torch.Size([6894, 60, 60, 12]), type torch.uint8, and labels of shape torch.Size([6894, 1]), type torch.int32.\n",
      "一共有：128个图片\n",
      "Net(\n",
      "  (conv1): Conv2d(12, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=7200, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=2, bias=True)\n",
      ")\n",
      "Accuracy of the network on the test set: 66.391%\n",
      "测试时间为：0:00:00.000063\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "#import h5py_cache as h5c\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "import torch.multiprocessing\n",
    "import sys\n",
    "import time\n",
    "#torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "from hdf5_dataset import H5Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_path = './data/for_train.h5'\n",
    "test_data_path ='./data/for_test.h5'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the test data.\n",
    "print('Loading test data ...')\n",
    "test_set = H5Dataset(test_data_path, 0, 10000)\n",
    "test_loader = torchdata.DataLoader(test_set, batch_size=128, shuffle=True)\n",
    "data_iter = iter(test_loader)\n",
    "images, labels = next(data_iter)\n",
    "num=len(labels)\n",
    "print('一共有：{}个图片'.format(num))\n",
    "\n",
    "CHANNELS = [20, 50, 500]\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, CHANNELS[0], 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(CHANNELS[0], CHANNELS[1], 5)\n",
    "        self.fc1 = nn.Linear(CHANNELS[1] * 12 * 12, CHANNELS[2])\n",
    "        self.fc2 = nn.Linear(CHANNELS[2], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net=Net(12)\n",
    "net.load_state_dict(torch.load('/home/wuxr/graspping/gpd-master/pytorch/gpd_orgin/final_model/lr0001_64/model.pwf'))\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(net)\n",
    "net.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        a=datetime.now()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)#输出预测的类别。1表示行的最大值。\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.long()).sum().item()    #统计标签与预测相同的个数。\n",
    "b=datetime.now()\n",
    "\n",
    "aver_time = (b-a)/num\n",
    "accuracy = 100.0 * float(correct) / float(total)\n",
    "print('Accuracy of the network on the test set: %.3f%%' % (\n",
    "    accuracy))\n",
    "print('测试时间为：{}'.format(aver_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab775025",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ninet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57484482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:43:56.823864Z",
     "start_time": "2021-04-28T12:43:48.134914Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Loading test data ...\n",
      "Loaded images of shape torch.Size([6894, 60, 60, 12]), type torch.uint8, and labels of shape torch.Size([6894, 1]), type torch.int32.\n",
      "一共有：64个图片\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(12, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): BatchNorm()\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm()\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Sequential(\n",
      "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm()\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Dropout(p=0.5, inplace=False)\n",
      "  (7): Sequential(\n",
      "    (0): Conv2d(384, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm()\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (8): GlobalAvgPool2d()\n",
      "  (9): FlattenLayer()\n",
      ")\n",
      "Accuracy of the network on the test set: 64.128%\n",
      "测试时间为：0:00:00.000210\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "#import h5py_cache as h5c\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "import torch.multiprocessing\n",
    "import sys\n",
    "import time\n",
    "#torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "from hdf5_dataset import H5Dataset\n",
    "\n",
    "train_data_path = './data/for_train.h5'\n",
    "test_data_path ='./data/for_test.h5'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the test data.\n",
    "print('Loading test data ...')\n",
    "test_set = H5Dataset(test_data_path, 0, 10000)\n",
    "test_loader = torchdata.DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "data_iter = iter(test_loader)\n",
    "images, labels = next(data_iter)\n",
    "num=len(labels)\n",
    "print('一共有：{}个图片'.format(num))\n",
    "\n",
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training,\n",
    "                                                          X, self.gamma, self.beta, self.moving_mean,\n",
    "                                                          self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n",
    "\n",
    "\n",
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                        BatchNorm(out_channels, num_dims=4),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU())\n",
    "    return blk\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):  # x shape: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nin_block(12, 96, kernel_size=11, stride=4, padding=0),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 2, kernel_size=3, stride=1, padding=1),\n",
    "    GlobalAvgPool2d(),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "    FlattenLayer())\n",
    "\n",
    "net=net\n",
    "\n",
    "net.load_state_dict(torch.load('/home/wuxr/graspping/gpd-master/pytorch/gpd_orgin/final_model/lr0001_ninnet_bn_200epoch_00005lr/model_65.12909776617349.pwf'))\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(net)\n",
    "net.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        a=datetime.now()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)#输出预测的类别。1表示行的最大值。\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.long()).sum().item()    #统计标签与预测相同的个数。\n",
    "b=datetime.now()\n",
    "\n",
    "aver_time = (b-a)/num\n",
    "accuracy = 100.0 * float(correct) / float(total)\n",
    "print('Accuracy of the network on the test set: %.3f%%' % (\n",
    "    accuracy))\n",
    "print('测试时间为：{}'.format(aver_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0bf40",
   "metadata": {},
   "source": [
    "# one_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820d8a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T13:01:38.195512Z",
     "start_time": "2021-04-28T13:01:33.212887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Loading test data ...\n",
      "Loaded images of shape torch.Size([6894, 60, 60, 12]), type torch.uint8, and labels of shape torch.Size([6894, 1]), type torch.int32.\n",
      "一共有：128个图片\n",
      "one_fl_Net_con_nomal(\n",
      "  (a_1): Conv2d(12, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "  (batch_1): BatchNorm()\n",
      "  (a_one): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (b_1): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (batch_2): BatchNorm()\n",
      "  (b_one): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (c_1): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batch_3): BatchNorm()\n",
      "  (c_one): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (consoft): Conv2d(384, 50, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=7200, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n",
      "Accuracy of the network on the test set: 68.639%\n",
      "测试时间为：0:00:00.000225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "#import h5py_cache as h5c\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "import torch.multiprocessing\n",
    "import sys\n",
    "import time\n",
    "#torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "from hdf5_dataset import H5Dataset\n",
    "\n",
    "train_data_path = './data/for_train.h5'\n",
    "test_data_path ='./data/for_test.h5'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the test data.\n",
    "print('Loading test data ...')\n",
    "test_set = H5Dataset(test_data_path, 0, 10000)\n",
    "test_loader = torchdata.DataLoader(test_set, batch_size=128, shuffle=True)\n",
    "data_iter = iter(test_loader)\n",
    "images, labels = next(data_iter)\n",
    "num=len(labels)\n",
    "print('一共有：{}个图片'.format(num))\n",
    "\n",
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training,\n",
    "                                                          X, self.gamma, self.beta, self.moving_mean,\n",
    "                                                          self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n",
    "class one_fl_Net_con_nomal(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(one_fl_Net_con_nomal, self).__init__()\n",
    "\n",
    "        self.a_1 = nn.Conv2d(12, 96, kernel_size=11, stride=4, padding=0)\n",
    "        self.batch_1 = BatchNorm(96, num_dims=4)\n",
    "        self.a_one = nn.Conv2d(96, 96, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.b_1 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.batch_2 = BatchNorm(256, num_dims=4)\n",
    "        self.b_one = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.c_1 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_3 = BatchNorm(384, num_dims=4)\n",
    "        self.c_one = nn.Conv2d(384, 384, kernel_size=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.consoft = nn.Conv2d(384, 50, kernel_size=1, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(50 * 12 * 12, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch_1(self.a_1(x)))\n",
    "        x = F.relu(self.a_one(F.relu(self.a_one(x))))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.batch_2(self.b_1(x)))\n",
    "        x = F.relu(self.b_one(F.relu(self.b_one(x))))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.batch_3(self.c_1(x)))\n",
    "        x = F.relu(self.c_one(F.relu(self.c_one(x))))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.consoft(x)\n",
    "\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2] * x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "net = one_fl_Net_con_nomal(12)\n",
    "\n",
    "net.load_state_dict(torch.load('/home/wuxr/graspping/gpd-master/pytorch/gpd_orgin/final_model/lr00005_64_one_fl_conv_batch/model_70.01740644038294.pwf'))\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(net)\n",
    "net.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        a=datetime.now()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)#输出预测的类别。1表示行的最大值。\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.long()).sum().item()    #统计标签与预测相同的个数。\n",
    "b=datetime.now()\n",
    "\n",
    "aver_time = (b-a)/num\n",
    "accuracy = 100.0 * float(correct) / float(total)\n",
    "print('Accuracy of the network on the test set: %.3f%%' % (\n",
    "    accuracy))\n",
    "print('测试时间为：{}'.format(aver_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d47ebcf",
   "metadata": {},
   "source": [
    "# fc2nin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64060620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T12:42:39.092745Z",
     "start_time": "2021-04-28T12:42:35.015155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Loading test data ...\n",
      "Loaded images of shape torch.Size([6894, 60, 60, 12]), type torch.uint8, and labels of shape torch.Size([6894, 1]), type torch.int32.\n",
      "一共有：64个图片\n",
      "Sequential(\n",
      "  (0): Conv2d(12, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Sequential(\n",
      "    (0): Conv2d(50, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm()\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (7): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Sequential(\n",
      "    (0): Conv2d(384, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm()\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (10): GlobalAvgPool2d()\n",
      "  (11): FlattenLayer()\n",
      ")\n",
      "Accuracy of the network on the test set: 66.913%\n",
      "测试时间为：0:00:00.000143\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "#import h5py_cache as h5c\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torchdata\n",
    "import torch.multiprocessing\n",
    "import sys\n",
    "import time\n",
    "#torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "from hdf5_dataset import H5Dataset\n",
    "\n",
    "train_data_path = './data/for_train.h5'\n",
    "test_data_path ='./data/for_test.h5'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load the test data.\n",
    "print('Loading test data ...')\n",
    "test_set = H5Dataset(test_data_path, 0, 10000)\n",
    "test_loader = torchdata.DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "data_iter = iter(test_loader)\n",
    "images, labels = next(data_iter)\n",
    "num=len(labels)\n",
    "print('一共有：{}个图片'.format(num))\n",
    "\n",
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层的情况，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "        # 训练模式下用当前的均值和方差做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.zeros(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(self.training,\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n",
    "##############           net\n",
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                        BatchNorm(out_channels, num_dims=4),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "                        nn.ReLU())\n",
    "    return blk\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):  # x shape: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    #     nin_block(12, 96, kernel_size=11, stride=4, padding=0),\n",
    "    #     nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    #     nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "\n",
    "    #     nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nn.Conv2d(12, 20, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(20, 50, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "\n",
    "    nin_block(50, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 2, kernel_size=3, stride=1, padding=1),\n",
    "    GlobalAvgPool2d(),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "    FlattenLayer())\n",
    "\n",
    "\n",
    "\n",
    "net = net\n",
    "\n",
    "net.load_state_dict(torch.load('/home/wuxr/graspping/gpd-master/pytorch/gpd_orgin/final_model/lr00005_fc2nin_bn_200epoch_3/model_68.13170873223092.pwf'))\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(net)\n",
    "net.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        a=datetime.now()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)#输出预测的类别。1表示行的最大值。\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.long()).sum().item()    #统计标签与预测相同的个数。\n",
    "b=datetime.now()\n",
    "\n",
    "aver_time = (b-a)/num\n",
    "accuracy = 100.0 * float(correct) / float(total)\n",
    "print('Accuracy of the network on the test set: %.3f%%' % (\n",
    "    accuracy))\n",
    "print('测试时间为：{}'.format(aver_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e752ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
